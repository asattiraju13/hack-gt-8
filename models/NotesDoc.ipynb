{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26bca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stop(string):\n",
    "    new = []\n",
    "    for i in string.split(' '):\n",
    "        if i not in stop:\n",
    "            new.append(i)\n",
    "    return ' '.join(new)\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    s1 = nlp(remove_stop(s1))\n",
    "    s2 = nlp(remove_stop(s2))\n",
    "    return s1.similarity(s2)\n",
    "    # implement later 3dspin kekw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab3319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_5250/3195911075.py:20: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return s1.similarity(s2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.45505968154209236,\n",
       " 0.02910383485151071,\n",
       " 0.0710402159449818,\n",
       " 0.3174145908129003,\n",
       " 0.5172356477080532,\n",
       " 0.5427175051052847]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"This is my sentence.\", \"It's kinda pog not gonna lie\", \"I love potatoes\"]\n",
    "b = [\"I enjoy large potatoes.\", \"They make good fries\"]\n",
    "out = []\n",
    "for j in a:\n",
    "    for k in b:\n",
    "        out.append(similarity(j, k))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68012037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'and', 'wonder', 'wonder', 'and', 'think']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence  = \"Think and wonder, wonder and think.\".lower()\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e82d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesDoc:\n",
    "    def __init__(self, string):\n",
    "        sentences = string.split(\". \")\n",
    "        self.raw_sentences = sentences\n",
    "        processed_sentences = []\n",
    "        for i, v in enumerate(sentences):\n",
    "            processed_sentences.append(' '.join(tokenizer.tokenize(remove_stop(v.lower()))))\n",
    "#         print(sentences)\n",
    "        self.processed_sentences = processed_sentences\n",
    "        \n",
    "    def update(self, new_string):\n",
    "        raw_new_sentences = new_string.split(\". \")\n",
    "        new_sentences = []\n",
    "        for i, v in enumerate(raw_new_sentences):\n",
    "            new_sentences.append(' '.join(tokenizer.tokenize(remove_stop(v.lower()))))\n",
    "        print(new_sentences)\n",
    "        similarities = [[0 for i in range(len(new_sentences))] for j in range(len(self.processed_sentences))]\n",
    "        for i, v in enumerate(self.processed_sentences):\n",
    "            for j, w in enumerate(new_sentences):\n",
    "                similarities[i][j] = similarity(v, w)\n",
    "        print(similarities)\n",
    "        similar_pairs = []\n",
    "        js = {}\n",
    "        for i in range(len(similarities)):\n",
    "            for j, s in enumerate(similarities[i]):\n",
    "                if s >= CUTOFF:\n",
    "                    similar_pairs.append((i, j))\n",
    "                    if j not in js:\n",
    "                        js[j] = i\n",
    "       \n",
    "        \"\"\"\n",
    "        TODO: Make it add it at the proper point\n",
    "        \"\"\"\n",
    "        last = [-1 for i in range(len(new_sentences))]\n",
    "        for j in range(len(new_sentences)):\n",
    "            if j in js:\n",
    "                last[j] = j\n",
    "            else:\n",
    "                last[j] = -1 if j==0 else last[j-1]\n",
    "        \n",
    "        \n",
    "        final_raw = self.raw_sentences\n",
    "        final_processed = self.processed_sentences\n",
    "        \n",
    "        \n",
    "        for j in range(len(new_sentences)):\n",
    "            if j not in js:\n",
    "                posn = 0\n",
    "                if last[j] != -1:\n",
    "                    posn = js[last[j]]\n",
    "                final_raw.insert(posn, raw_new_sentences[j])\n",
    "                final_processed.insert(posn, new_sentences[j])\n",
    "        self.raw_sentences = final_raw\n",
    "        self.processed_sentences = final_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef479f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NotesDoc(\"\"\"We are interested in manipulating freely moving cables, in real time, with a pair of robotic grippers, and with no added mechanical constraints. The main contribution of this paper is a perception and control framework that moves in that direction, and uses real-time tactile feedback to accomplish the task of following a dangling cable. The approach relies on a vision-based tactile sensor, GelSight, that estimates the pose of the cable in the grip, and the friction forces during cable sliding.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30f40d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['successful implementation tactile perception model based controller cable following task generalization different cables different following velocities demonstrates possible use simple models controllers manipulate deformable objects', 'illustrative demonstration picking finding end headphone cable insertion provides example proposed framework play role practical cable related manipulation tasks']\n",
      "[[0.8385802911509959, 0.7465409958237502], [0.8466981995897297, 0.9084417717593555], [0.7576879889461564, 0.7768383895551874]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_5250/3195911075.py:20: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return s1.similarity(s2)\n"
     ]
    }
   ],
   "source": [
    "a.update(\"\"\"The successful implementation of the tactile perception and model-based controller in the cable following task, and its generalization to different cables and to different following velocities, demonstrates that it is possible to use simple models and controllers to manipulate deformable objects. The illustrative demonstration of picking and finding the end of a headphone cable for insertion provides a example of how the proposed framework can play a role in practical cable-related manipulation tasks\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dbcdcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The successful implementation of the tactile perception and model-based controller in the cable following task, and its generalization to different cables and to different following velocities, demonstrates that it is possible to use simple models and controllers to manipulate deformable objects                  We are interested in manipulating freely moving cables, in real time, with a pair of robotic grippers, and with no added mechanical constraints                  The main contribution of this paper is a perception and control framework that moves in that direction, and uses real-time tactile feedback to accomplish the task of following a dangling cable                  The approach relies on a vision-based tactile sensor, GelSight, that estimates the pose of the cable in the grip, and the friction forces during cable sliding.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'                  '.join(a.raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c2d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
