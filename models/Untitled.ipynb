{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f26bca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    s1 = nlp(remove_stop(s1))\n",
    "    s2 = nlp(remove_stop(s2))\n",
    "    return s1.similarity(s2)\n",
    "    # implement later 3dspin kekw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ab3319d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rempve_stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_13243/3972757564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_13243/1021351225.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(s1, s2)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrempve_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rempve_stop' is not defined"
     ]
    }
   ],
   "source": [
    "a = [\"This is my sentence.\", \"It's kinda pog not gonna lie\", \"I love potatoes\"]\n",
    "b = [\"I enjoy large potatoes.\", \"They make good fries\"]\n",
    "out = []\n",
    "for j in a:\n",
    "    for k in b:\n",
    "        out.append(similarity(j, k))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "68012037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(string):\n",
    "    new = []\n",
    "    for i in string.split(' '):\n",
    "        if i not in stop:\n",
    "            new.append(i)\n",
    "    return ' '.join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "24e82d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7ee609d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotesDoc:\n",
    "    def __init__(self, string):\n",
    "        sentences = string.split(\". \")\n",
    "        self.raw_sentences = sentences\n",
    "        processed_sentences = []\n",
    "        for i, v in enumerate(sentences):\n",
    "            processed_sentences.append(' '.join(tokenizer.tokenize(remove_stop(v.lower()))))\n",
    "#         print(sentences)\n",
    "        self.processed_sentences = processed_sentences\n",
    "        \n",
    "    def update(self, new_string):\n",
    "        raw_new_sentences = new_string.split(\". \")\n",
    "        new_sentences = []\n",
    "        for i, v in enumerate(raw_new_sentences):\n",
    "            new_sentences.append(' '.join(tokenizer.tokenize(remove_stop(v.lower()))))\n",
    "        print(new_sentences)\n",
    "        similarities = [[0 for i in range(len(new_sentences))] for j in range(len(self.processed_sentences))]\n",
    "        for i, v in enumerate(self.processed_sentences):\n",
    "            for j, w in enumerate(new_sentences):\n",
    "                similarities[i][j] = similarity(v, w)\n",
    "        print(similarities)\n",
    "        similar_pairs = []\n",
    "        js = set()\n",
    "        for i in range(len(similarities)):\n",
    "            for j, s in enumerate(similarities[i]):\n",
    "                if s >= CUTOFF:\n",
    "                    similar_pairs.append((i, j, s))\n",
    "                    js.add(j)\n",
    "        \"\"\"\n",
    "        TODO: Make it add it at the proper point\n",
    "        \"\"\"\n",
    "        final_raw = []\n",
    "        final_processed = []\n",
    "        for j in range(len(new_sentences)):\n",
    "            if j not in js:\n",
    "                final_raw.append(raw_new_sentences[j])\n",
    "                final_processed.append(new_sentences[j])\n",
    "        self.raw_sentences += final_raw\n",
    "        self.processed_sentences += final_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac49d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NotesDoc(\"This is my sentence. It's kinda pog not gonna lie. I love potatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3c6a57e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is my sentence', \"It's kinda pog not gonna lie\", 'I love potatoes']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a97d8eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence', 'kinda pog gonna lie', 'love potatoes']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "605d14ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy large potatoes', 'make good fries']\n",
      "[[0.2510399326348809, 0.11463994263706957], [0.13458987309203674, 0.21935153896124576], [0.33491347443181146, 0.24493482191976756]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_13243/1960745192.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return s1.similarity(s2)\n"
     ]
    }
   ],
   "source": [
    "a.update(\"I enjoy large potatoes. They make good fries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0de52f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'kinda pog gonna lie', 'love potatoes', 'make good fries']\n"
     ]
    }
   ],
   "source": [
    "print(a.processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "be42cc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is my sentence',\n",
       " \"It's kinda pog not gonna lie\",\n",
       " 'I love potatoes',\n",
       " 'They make good fries']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d75c7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bab945b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d722f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's kinda pog gonna lie\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It's kinda pog not gonna lie\"\n",
    "new = []\n",
    "for i in sentence.split(' '):\n",
    "    if i not in stop:\n",
    "        new.append(i)\n",
    "print(' '.join(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c23809a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'and', 'wonder', 'wonder', 'and', 'think']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sentence  = \"Think and wonder, wonder and think.\".lower()\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5b96da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30706015606706805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bc/r3f8htg14cdb8gyz7p8sl5qm0000gn/T/ipykernel_13243/1960745192.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return s1.similarity(s2)\n"
     ]
    }
   ],
   "source": [
    "print(similarity(\"Eigenvalues can eat my balls\".lower(), \"This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b4fcc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may happen using one small models, e.g. `en_core_web_sm`, ship word vectors use context-sensitive tensors. You always add word vectors, use one larger models instead\n"
     ]
    }
   ],
   "source": [
    "print(remove_stop(\"This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb1f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
